{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8AiCbTZnIFiu"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Eala7caYp8N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XywBEYGnIa2n",
        "outputId": "015f7d6f-2a7f-4a00-af84-fc24c7780c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoHgtFLLXC2e",
        "outputId": "e973f23b-6588-427d-ef25-8c09e4350652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: cd: ASD: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! cd ASD && ls | wc -l\n",
        "asd_files = os.listdir('/content/drive/MyDrive/Speech/FinalData/ASD/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWd8hvO4XdfS",
        "outputId": "42218e51-dbdb-42f6-b448-3be9d3474856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: cd: TD: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! cd TD && ls | wc -l\n",
        "td_files = os.listdir('/content/drive/MyDrive/Speech/FinalData/TD/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr5QXnaWb5CY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BlhVfsN2XxG"
      },
      "outputs": [],
      "source": [
        "import numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYYa2GWAEQtL",
        "outputId": "4dfb4d32-a674-4189-94d4-da5c7dbd582e"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0NrRq2eEW0E"
      },
      "outputs": [],
      "source": [
        "sentence = 'i am preprocessing the text'\n",
        "words = nltk.word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuIbDfqegHLl",
        "outputId": "5a3a5970-f8a6-48ba-c11d-7fc6c8d88b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Speech/FinalData\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Speech/FinalData/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mm-P8QdC-xq"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('childesData.csv', index_col = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHWgTIS5gVDE",
        "outputId": "27fea6b9-6d2d-4cae-a3bf-27819ce7a090"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-682a1941-a9d6-4510-84f2-f63b5d9980df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>ChildText</th>\n",
              "      <th>Label</th>\n",
              "      <th>ChildTheraText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>060622.cha</td>\n",
              "      <td>these are Brett's . | drop them off . | do wi...</td>\n",
              "      <td>0</td>\n",
              "      <td>what do you do ? | mix the stuff in . $ right...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>030900.cha</td>\n",
              "      <td>want lunch . | want some lunch . | have some ...</td>\n",
              "      <td>0</td>\n",
              "      <td>roger want some lunch ? | want lunch . $ let'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>080506.cha</td>\n",
              "      <td>hi , Helen . | hi Tina . | xxx . | hi Tina . ...</td>\n",
              "      <td>0</td>\n",
              "      <td>okay ? | hi , Helen . $ did you say hi to Tin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>080808.cha</td>\n",
              "      <td>nah . | don't do that . | no . | I don't care...</td>\n",
              "      <td>0</td>\n",
              "      <td>yeah , come on . | don't do that . $ come on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1025.cha</td>\n",
              "      <td>oh â€¡ I found a jelly+bean . | I wanna eat it ...</td>\n",
              "      <td>1</td>\n",
              "      <td>you found a jelly+bean . | I wanna eat it . $...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>040715.cha</td>\n",
              "      <td>come . | ready , go . | &amp;mm . | &amp;mm . | doggy...</td>\n",
              "      <td>0</td>\n",
              "      <td>what do you say ? | come . $ open it . | dogg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>111.cha</td>\n",
              "      <td>&amp;-um make a xxx and dump . | make dump . | bi...</td>\n",
              "      <td>1</td>\n",
              "      <td>a what ? | make dump . $ is that it ? | yeah ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>030806.cha</td>\n",
              "      <td>want water . | water . | sit down . | sit rig...</td>\n",
              "      <td>0</td>\n",
              "      <td>Stuart . | want water . $ you want water ? | ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>120.cha</td>\n",
              "      <td>beep@o beep@o . | &amp;ah xxx . | &amp;sh . | one . |...</td>\n",
              "      <td>0</td>\n",
              "      <td>hm| &amp;ah xxx . $ wow, yeah . | &amp;sh . $ yeah, c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>billy2.cha</td>\n",
              "      <td>ni(ght)_night . | ni(ght)_night . | are these...</td>\n",
              "      <td>1</td>\n",
              "      <td>where is the baby ? | ni(ght)_night . $ what ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-682a1941-a9d6-4510-84f2-f63b5d9980df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-682a1941-a9d6-4510-84f2-f63b5d9980df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-682a1941-a9d6-4510-84f2-f63b5d9980df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           ID                                          ChildText  Label  \\\n",
              "0  060622.cha   these are Brett's . | drop them off . | do wi...      0   \n",
              "1  030900.cha   want lunch . | want some lunch . | have some ...      0   \n",
              "2  080506.cha   hi , Helen . | hi Tina . | xxx . | hi Tina . ...      0   \n",
              "3  080808.cha   nah . | don't do that . | no . | I don't care...      0   \n",
              "4    1025.cha   oh â€¡ I found a jelly+bean . | I wanna eat it ...      1   \n",
              "5  040715.cha   come . | ready , go . | &mm . | &mm . | doggy...      0   \n",
              "6     111.cha   &-um make a xxx and dump . | make dump . | bi...      1   \n",
              "7  030806.cha   want water . | water . | sit down . | sit rig...      0   \n",
              "8     120.cha   beep@o beep@o . | &ah xxx . | &sh . | one . |...      0   \n",
              "9  billy2.cha   ni(ght)_night . | ni(ght)_night . | are these...      1   \n",
              "\n",
              "                                      ChildTheraText  \n",
              "0   what do you do ? | mix the stuff in . $ right...  \n",
              "1   roger want some lunch ? | want lunch . $ let'...  \n",
              "2   okay ? | hi , Helen . $ did you say hi to Tin...  \n",
              "3   yeah , come on . | don't do that . $ come on ...  \n",
              "4   you found a jelly+bean . | I wanna eat it . $...  \n",
              "5   what do you say ? | come . $ open it . | dogg...  \n",
              "6   a what ? | make dump . $ is that it ? | yeah ...  \n",
              "7   Stuart . | want water . $ you want water ? | ...  \n",
              "8   hm| &ah xxx . $ wow, yeah . | &sh . $ yeah, c...  \n",
              "9   where is the baby ? | ni(ght)_night . $ what ...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yOHJdVFEfH53"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JKCuY0WWfRhR"
      },
      "source": [
        "##ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJwNjMFpRKpM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "try:\n",
        "    import nltk\n",
        "except:\n",
        "    print(\"First install NLTK using pip install nltk command\")\n",
        "    exit()\n",
        "try:\n",
        "    import gensim\n",
        "    from gensim import corpora, models, similarities\n",
        "except:\n",
        "    print(\"First install Gensim using pip install nltk command\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "class Stemmer:\n",
        "    w2vModel = None\n",
        "    sensitivity = 10\n",
        "\n",
        "    # constructor\n",
        "    def __init__(self, modelLocation= \"w2vModel\", w2vModel = None):\n",
        "        try:\n",
        "            self.w2vModel = gensim.models.Word2Vec.load(modelLocation)\n",
        "        except:\n",
        "            print(\"Could not locate the w2vModel file in the directory : \"+(modelLocation))\n",
        "            print(\"Try to load the w2vModel and try again\")\n",
        "\n",
        "\n",
        "\n",
        "    #  ----------- Stemming functions -----------\n",
        "\n",
        "\n",
        "    # takes a word and removes the repeated occurance of characters in that word\n",
        "    # outputs word without repeat consecutive occurance of the word\n",
        "    def RepetitionStemmer(self, word):\n",
        "        # find repeted occurence of letters in a word\n",
        "        # remove the occurence\n",
        "        i=0\n",
        "        newWord = ''\n",
        "        while(i <len(word)):\n",
        "            c = word[i]\n",
        "            newWord+=c\n",
        "            while(i<len(word) and word[i] == c):\n",
        "                i=i+1\n",
        "\n",
        "        return newWord\n",
        "\n",
        "    # takes a word2vec model, word and nWords(to run most similar on - higher the better but slower)\n",
        "    # output the list of words similar to that word ( including that word passed through repetition stemmer)\n",
        "    def WordEmbeddingStemmer(self, w2vModel, word, nWords = 10):\n",
        "\n",
        "        try:\n",
        "            similarWordsList =[w2vModel.wv.most_similar(word, topn = nWords )[i][0] for i in range(10)]\n",
        "        except:\n",
        "            return self.RepetitionStemmer(word)\n",
        "\n",
        "        word = self.RepetitionStemmer(word)\n",
        "\n",
        "        outputList = []\n",
        "        for similarWord in similarWordsList:\n",
        "            stemmSimilarWord = self.RepetitionStemmer(similarWord)\n",
        "            w0 = word\n",
        "            w1 = word[:-1]\n",
        "            sw0 = stemmSimilarWord\n",
        "            sw1 = stemmSimilarWord[:-1]\n",
        "\n",
        "            if (sw0 in w0) or( w0 in sw0) or (sw1 in w0) or (w1 in sw0):\n",
        "                if(len(stemmSimilarWord)<len(word)):\n",
        "                    outputList.append(stemmSimilarWord)\n",
        "                else:\n",
        "                    outputList.append(word)\n",
        "        if len(outputList) == 0:\n",
        "            outputList.append(word)\n",
        "        return outputList[0]\n",
        "\n",
        "    # stemmers\n",
        "    def stemWord(self, word):\n",
        "        return self.WordEmbeddingStemmer(self.w2vModel, word)\n",
        "\n",
        "    def stemListOfWords(self, listOfWords):\n",
        "        return [self.WordEmbeddingStemmer(self.w2vModel, word) for word in listOfWords]\n",
        "\n",
        "    def stem2dListOfWords(self, listOfWords2d):\n",
        "        output = []\n",
        "        for sentenceOfWords in listOfWords2d:\n",
        "            output.append([self.WordEmbeddingStemmer(self.w2vModel, word) for word in sentenceOfWords])\n",
        "        return output\n",
        "myStemmer = Stemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6YuoCwkSSwI"
      },
      "outputs": [],
      "source": [
        "def stem(sentence):\n",
        "  words = sentence.split()\n",
        "  for i in range(len(words)):\n",
        "    words[i] = myStemmer.stemWord(words[i])\n",
        "  return words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FHzmaxyfVDY"
      },
      "outputs": [],
      "source": [
        "df_hindi = pd.read_csv('childesData_hinglish.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOaC5EfupurR"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "import re\n",
        "\n",
        "def n_grams(text):\n",
        "  sentences = re.split(\" .  ? \",text)\n",
        "  uni=[]\n",
        "  bi=[]\n",
        "  for sentence in sentences:\n",
        "    stemmed = stem(sentence)\n",
        "    unigrams = ngrams(stemmed, 1)\n",
        "    bigrams= ngrams(stemmed, 2)\n",
        "    uni.extend(unigrams)\n",
        "    bi.extend(bigrams)\n",
        "  uni_sorted=sorted_grams(uni,1)\n",
        "  bi_sorted=sorted_grams(bi,2)\n",
        "  return uni_sorted, bi_sorted\n",
        "\n",
        "def sorted_grams(ngrams, n=2):\n",
        "    counts = {}\n",
        "    for ngram in ngrams:\n",
        "        counts[ngram] = counts.get(ngram, 0) + 1\n",
        "\n",
        "    return sorted(((v,k) for k,v in counts.items()), reverse=True)\n",
        "\n",
        "all_text = ''\n",
        "\n",
        "for i in range(len(df_hindi['ChildText'])):\n",
        "    if(type(df_hindi['ChildText'][i]) == float):\n",
        "        df_hindi['ChildText'][i] = \"\"\n",
        "    cur = []\n",
        "    cunits = list(df_hindi['ChildText'][i].split('.'))\n",
        "    tunits = list(df_hindi['ChildTheraText'][i].split('$'))\n",
        "    text = df_hindi['ChildText'][i].replace('.', '').replace('  ',' ')\n",
        "    all_text += text\n",
        "\n",
        "uni_sorted,bi_sorted= n_grams(all_text)\n",
        "print(uni_sorted)\n",
        "print(bi_sorted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5sJ0DxYRpEj"
      },
      "outputs": [],
      "source": [
        "## unigrams = hai, ha, h, ye, nahin, aur #frequency above 40\n",
        "## bigrams = (ha, ha), (hota, h), (ho, gya), (yeh, rhi), (raha, hai) #frequency above 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gU1rw1A9M4K7"
      },
      "outputs": [],
      "source": [
        "ngram_featureset=[]\n",
        "for i in range(len(df_hindi['ChildText'])):\n",
        "    counts=[0,0,0,0,0,0,0,0,0,0,0]\n",
        "    if(type(df_hindi['ChildText'][i]) == float):\n",
        "        df_hindi['ChildText'][i] = \"\"\n",
        "    cur = []\n",
        "    cunits = list(df_hindi['ChildText'][i].split('.'))\n",
        "    tunits = list(df_hindi['ChildTheraText'][i].split('$'))\n",
        "    text = df_hindi['ChildText'][i].replace('.', '').replace('  ',' ')\n",
        "    uni_sorted,bi_sorted= n_grams(text)\n",
        "    for unigram in uni_sorted:\n",
        "      if (unigram[1][0]=='se'):\n",
        "        counts[0] = unigram[0]\n",
        "      if (unigram[1][0]=='aur'):\n",
        "        counts[1] = unigram[0]\n",
        "      if (unigram[1][0]=='aise'):\n",
        "        counts[2] = unigram[0]\n",
        "      if ((unigram[1][0]=='ye' or (unigram[1][0]=='yeh'))):\n",
        "        counts[3] = unigram[0]\n",
        "      if (unigram[1][0]=='nahin'):\n",
        "        counts[4] = unigram[0]\n",
        "    for bigram in bi_sorted:\n",
        "      if ((bigram[1][0]=='ha') and (bigram[1][1]=='ha')):\n",
        "        counts[5] = bigram[0]\n",
        "      if ((bigram[1][0]=='hota') and (bigram[1][1]=='hai')):\n",
        "        counts[5] = bigram[0]\n",
        "      if ((bigram[1][0]=='ho') and (bigram[1][1]=='gya')):\n",
        "        counts[5] = bigram[0]\n",
        "      if ((bigram[1][0]=='yeh') and (bigram[1][1]=='rhi')):\n",
        "        counts[5] = bigram[0]\n",
        "      if ((bigram[1][0]=='rha') and (bigram[1][1]=='hai')):\n",
        "        counts[5] = bigram[0]\n",
        "    counts[10] = df_hindi['Label'][i]\n",
        "    ngram_featureset.append(counts)\n",
        "print(ngram_featureset[:][:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6pnL8LRUyum"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(ngram_featureset).to_csv('features_ngram.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VilK-BqJOJs9"
      },
      "source": [
        "##opensmile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz-lRWTFOTes",
        "outputId": "2ecec70c-f787-4707-98e8-2b87eb777eeb"
      },
      "outputs": [],
      "source": [
        "!pip install opensmile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flPiI-chOW_6"
      },
      "outputs": [],
      "source": [
        "import opensmile\n",
        "\n",
        "smile = opensmile.Smile(\n",
        "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "    feature_level=opensmile.FeatureLevel.Functionals,\n",
        "    sampling_rate = 44100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NKaASeZY6-y"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame()\n",
        "#axis0\n",
        "#pd.concat([df1, df2], axis=0)\n",
        "files = ['0','1','2','3','4','5','6','7','8','9','10','12','15','16','17','18']\n",
        "for fname in files:\n",
        "  y = smile.process_file('ASD_hinglish/' + str(fname)+'child.wav')\n",
        "  y[\"label\"] = [0]\n",
        "  if (fname == '0'):\n",
        "    data = y\n",
        "  else:\n",
        "    data = pd.concat([data,y], axis = 0)\n",
        "  data.head()\n",
        "\n",
        "files = ['535','536','537','538','539','540','541','542','544','545','550','551','552','553','554','594','9531','9532']\n",
        "for fname in files:\n",
        "  y = smile.process_file('TD_hinglish/' + str(fname)+'child.wav')\n",
        "  y[\"label\"] = [1]\n",
        "  data = pd.concat([data,y], axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apkVL7yPR0Nm"
      },
      "outputs": [],
      "source": [
        "data.to_csv('features_acoustic.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V_9T-r_rj1Rc"
      },
      "source": [
        "##feature reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztSJQKWukKC2"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"features_acoustic.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9FbhXUt6fkm"
      },
      "outputs": [],
      "source": [
        "data = data.drop(['Unnamed: 0','Unnamed: 0.1','Unnamed: 0.2','file','start','end'],axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-xYyV-Hv8Nu"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "np.random.seed(42)\n",
        "\n",
        "# Specify the number of folds\n",
        "num_folds = 5\n",
        "\n",
        "# Initialize the cross-validator\n",
        "kf = KFold(n_splits=num_folds, random_state=42, shuffle = True)\n",
        "\n",
        "# Create lists to store the training and testing datasets for each fold\n",
        "train_datasets = []\n",
        "test_datasets = []\n",
        "\n",
        "# Split the data into folds and store the indices\n",
        "for train_index, test_index in kf.split(data):\n",
        "    # Obtain the training and testing datasets based on the indices\n",
        "    train_data = data.iloc[train_index]\n",
        "    test_data = data.iloc[test_index]\n",
        "\n",
        "    # Append the datasets to the lists\n",
        "    train_datasets.append(train_data)\n",
        "    test_datasets.append(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_eOR55xzTrhZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns # For pairplots and heatmaps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_correlation(df):\n",
        "    r = df.corr(method=\"spearman\")\n",
        "    plt.figure(figsize=(90,90))\n",
        "    heatmap = sns.heatmap(df.corr(), vmin=-1,\n",
        "                      vmax=1, annot=True)\n",
        "    plt.title(\"Spearman Correlation\")\n",
        "    return(r)\n",
        "\n",
        "correlation = display_correlation(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STp4e0ltVS51"
      },
      "outputs": [],
      "source": [
        "spearman = correlation.sort_values('label')['label']\n",
        "one = (spearman[0:16]).to_frame()\n",
        "two = (spearman[-2:]).to_frame()\n",
        "selected_features_list = pd.concat([one, two], axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BjXjsKniRwy"
      },
      "outputs": [],
      "source": [
        "selected_features = list(selected_features_list.index)\n",
        "print(spearman)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kLEMXy0it3L"
      },
      "outputs": [],
      "source": [
        "selected_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qly4kGITjdAT"
      },
      "outputs": [],
      "source": [
        "data_new = data[['unintell_prop',\n",
        " 'discourse2',\n",
        " 'unexpected_words',\n",
        " 'avg_sentence',\n",
        " 'ADP',\n",
        " 'ADJ']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1zkSV-2k6XR"
      },
      "outputs": [],
      "source": [
        "data_new.to_csv('text_selected.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vmB7UybvxgAD"
      },
      "source": [
        "###Fold1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0XxjKUgxjgP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns # For pairplots and heatmaps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_correlation(df):\n",
        "    r = df.corr(method=\"spearman\")\n",
        "    # plt.figure(figsize=(90,90))\n",
        "    # heatmap = sns.heatmap(df.corr(), vmin=-1,\n",
        "    #                   vmax=1, annot=True)\n",
        "    # plt.title(\"Spearman Correlation\")\n",
        "    return(r)\n",
        "correlation = display_correlation(train_datasets[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7gZY1L2xy2d",
        "outputId": "cbdba0a5-8c18-47f5-a2a8-a618209216e9"
      },
      "outputs": [],
      "source": [
        "spearman = correlation.sort_values('label')['label']\n",
        "for i in range(len(spearman)):\n",
        "  if spearman[i]<0:\n",
        "    spearman[i] = -spearman[i]\n",
        "spearman = spearman.sort_values()\n",
        "print(spearman)\n",
        "two = (spearman[-16:]).to_frame()\n",
        "selected_features_list = two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ONJ86UFzTuS",
        "outputId": "a01c798a-49db-43f3-c430-90106e5fb9d3"
      },
      "outputs": [],
      "source": [
        "selected_features_1 = list(selected_features_list.index)\n",
        "print(selected_features_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mior-sVT1gaD"
      },
      "outputs": [],
      "source": [
        "data_1 = data[selected_features_1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHjhrxlzAh7J"
      },
      "outputs": [],
      "source": [
        "data_1.to_csv('acoustic_1.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VrRJhXAJBqdf"
      },
      "source": [
        "###Fold2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuHHmv3qBqdq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns # For pairplots and heatmaps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_correlation(df):\n",
        "    r = df.corr(method=\"spearman\")\n",
        "    # plt.figure(figsize=(90,90))\n",
        "    # heatmap = sns.heatmap(df.corr(), vmin=-1,\n",
        "    #                   vmax=1, annot=True)\n",
        "    # plt.title(\"Spearman Correlation\")\n",
        "    return(r)\n",
        "correlation = display_correlation(train_datasets[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvYx7pRtBqdq",
        "outputId": "6d20a1e3-f0eb-4bd2-f25c-89218b563261"
      },
      "outputs": [],
      "source": [
        "spearman = correlation.sort_values('label')['label']\n",
        "for i in range(len(spearman)):\n",
        "  if spearman[i]<0:\n",
        "    spearman[i] = -spearman[i]\n",
        "spearman = spearman.sort_values()\n",
        "print(spearman)\n",
        "two = (spearman[-16:]).to_frame()\n",
        "selected_features_list = two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0A9JgTwBqdr",
        "outputId": "27d8feeb-13fc-4a59-8cfe-419cbca697e8"
      },
      "outputs": [],
      "source": [
        "selected_features_2 = list(selected_features_list.index)\n",
        "print(selected_features_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EX96mRYBqdr",
        "outputId": "2db3fe09-a545-48e1-c3ad-e3a915d2851c"
      },
      "outputs": [],
      "source": [
        "print(len(selected_features_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBzR2SFFBqdr"
      },
      "outputs": [],
      "source": [
        "data_2 = data[selected_features_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4Pf3fJWBqdr"
      },
      "outputs": [],
      "source": [
        "data_2.to_csv('acoustic_2.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZAOy9loCLHz"
      },
      "source": [
        "###Fold3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ0ay1jWCLH0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns # For pairplots and heatmaps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_correlation(df):\n",
        "    r = df.corr(method=\"spearman\")\n",
        "    # plt.figure(figsize=(90,90))\n",
        "    # heatmap = sns.heatmap(df.corr(), vmin=-1,\n",
        "    #                   vmax=1, annot=True)\n",
        "    # plt.title(\"Spearman Correlation\")\n",
        "    return(r)\n",
        "correlation = display_correlation(train_datasets[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIh3DU9xCLH0",
        "outputId": "085c519a-bfd0-413b-fc8f-a1c6bd38893a"
      },
      "outputs": [],
      "source": [
        "spearman = correlation.sort_values('label')['label']\n",
        "for i in range(len(spearman)):\n",
        "  if spearman[i]<0:\n",
        "    spearman[i] = -spearman[i]\n",
        "spearman = spearman.sort_values()\n",
        "print(spearman)\n",
        "two = (spearman[-16:]).to_frame()\n",
        "selected_features_list = two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3X8Pgp0CLH1",
        "outputId": "b8d3f641-c852-4c9f-e2f5-a6c83c107336"
      },
      "outputs": [],
      "source": [
        "selected_features_3 = list(selected_features_list.index)\n",
        "print(selected_features_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luq5xsFaCLH2"
      },
      "outputs": [],
      "source": [
        "data_3 = data[selected_features_3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wUzadiMCLH2"
      },
      "outputs": [],
      "source": [
        "data_3.to_csv('acoustic_3.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ESU-8jIFCgZm"
      },
      "source": [
        "###Fold4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioxA4kRICgZn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns # For pairplots and heatmaps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_correlation(df):\n",
        "    r = df.corr(method=\"spearman\")\n",
        "    # plt.figure(figsize=(90,90))\n",
        "    # heatmap = sns.heatmap(df.corr(), vmin=-1,\n",
        "    #                   vmax=1, annot=True)\n",
        "    # plt.title(\"Spearman Correlation\")\n",
        "    return(r)\n",
        "correlation = display_correlation(train_datasets[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILJhBKPqCgZo",
        "outputId": "de49eca7-275a-4d5f-a5a4-a7baa2d10bf8"
      },
      "outputs": [],
      "source": [
        "spearman = correlation.sort_values('label')['label']\n",
        "for i in range(len(spearman)):\n",
        "  if spearman[i]<0:\n",
        "    spearman[i] = -spearman[i]\n",
        "spearman = spearman.sort_values()\n",
        "print(spearman)\n",
        "two = (spearman[-16:]).to_frame()\n",
        "selected_features_list = two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkpOoEDHCgZo",
        "outputId": "70ed9aac-8a3b-4043-f08f-2f86ada29e9d"
      },
      "outputs": [],
      "source": [
        "selected_features_4 = list(selected_features_list.index)\n",
        "print(selected_features_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDgWC2GZCgZp"
      },
      "outputs": [],
      "source": [
        "data_4 = data[selected_features_4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg9xE_sQCgZp"
      },
      "outputs": [],
      "source": [
        "data_4.to_csv('acoustic_1.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7K_r6Sy0Ctnn"
      },
      "source": [
        "###Fold5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey2CoSzPCtnn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns # For pairplots and heatmaps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_correlation(df):\n",
        "    r = df.corr(method=\"spearman\")\n",
        "    # plt.figure(figsize=(90,90))\n",
        "    # heatmap = sns.heatmap(df.corr(), vmin=-1,\n",
        "    #                   vmax=1, annot=True)\n",
        "    # plt.title(\"Spearman Correlation\")\n",
        "    return(r)\n",
        "correlation = display_correlation(train_datasets[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WedC-GXwCtno",
        "outputId": "d8dc8cce-6fdd-4578-aa5c-684ba7534bf4"
      },
      "outputs": [],
      "source": [
        "spearman = correlation.sort_values('label')['label']\n",
        "for i in range(len(spearman)):\n",
        "  if spearman[i]<0:\n",
        "    spearman[i] = -spearman[i]\n",
        "spearman = spearman.sort_values()\n",
        "print(spearman)\n",
        "two = (spearman[-16:]).to_frame()\n",
        "selected_features_list = two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXeH6SMJCtno",
        "outputId": "b811c21a-03fa-4967-fec6-3d9610061331"
      },
      "outputs": [],
      "source": [
        "selected_features_5 = list(selected_features_list.index)\n",
        "print(selected_features_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HurYQO4eCtnp"
      },
      "outputs": [],
      "source": [
        "data_5 = data[selected_features_5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE1Iql32Ctnq"
      },
      "outputs": [],
      "source": [
        "data_5.to_csv('acoustic_5.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqz2S8XkGkDW"
      },
      "source": [
        "##Text Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a1OzV0CGuK-"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"features_hindi_labelled.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04qgZk_vHL2L"
      },
      "outputs": [],
      "source": [
        "data = data.drop(['Unnamed: 0'],axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSNN02jKGuLA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "num_folds = 5\n",
        "\n",
        "kf = KFold(n_splits=num_folds, random_state=42, shuffle = True)\n",
        "\n",
        "train_datasets = []\n",
        "test_datasets = []\n",
        "\n",
        "for train_index, test_index in kf.split(data):\n",
        "    train_data = data.iloc[train_index]\n",
        "    test_data = data.iloc[test_index]\n",
        "    train_datasets.append(train_data)\n",
        "    test_datasets.append(test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XpqbsF4xGn_3"
      },
      "source": [
        "###Folds definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p6qS-G_Gn_3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns # For pairplots and heatmaps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_correlation(df):\n",
        "    r = df.corr(method=\"spearman\")\n",
        "    # plt.figure(figsize=(90,90))\n",
        "    # heatmap = sns.heatmap(df.corr(), vmin=-1,\n",
        "    #                   vmax=1, annot=True)\n",
        "    # plt.title(\"Spearman Correlation\")\n",
        "    return(r)\n",
        "correlation = display_correlation(train_datasets[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR5_ig3mGn_4"
      },
      "outputs": [],
      "source": [
        "spearman = correlation.sort_values('label')['label']\n",
        "for i in range(len(spearman)):\n",
        "  if spearman[i]<0:\n",
        "    spearman[i] = -spearman[i]\n",
        "spearman = spearman.sort_values()\n",
        "print(spearman)\n",
        "two = (spearman[-16:]).to_frame()\n",
        "selected_features_list = two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgaNhlnLGn_5"
      },
      "outputs": [],
      "source": [
        "selected_features_1 = list(selected_features_list.index)\n",
        "print(selected_features_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Exyi2ZgUGn_6"
      },
      "outputs": [],
      "source": [
        "data_1 = data[selected_features_1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2yFH-cyGn_6"
      },
      "outputs": [],
      "source": [
        "data_1.to_csv('acoustic_1.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYollF3TjvMe"
      },
      "source": [
        "##Model and data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bTR-tSojVls"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7qibUMyGo3S"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcVGR3OheVol"
      },
      "outputs": [],
      "source": [
        "temp = pd.read_csv(\"acoustic_1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUFL3y2keSux"
      },
      "outputs": [],
      "source": [
        "acoustic1 = (pd.read_csv(\"acoustic_1.csv\").to_numpy())[:,1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w54UwqC0juP7"
      },
      "outputs": [],
      "source": [
        "text = pd.read_csv('text_selected.csv').to_numpy()\n",
        "ngram = pd.read_csv(\"features_ngram.csv\").to_numpy()\n",
        "acoustic = pd.read_csv(\"features_acoustic_selected.csv\").to_numpy()\n",
        "acoustic1 = (pd.read_csv(\"acoustic_1.csv\").to_numpy())[:,1:]\n",
        "acoustic2 = (pd.read_csv(\"acoustic_2.csv\").to_numpy())[:,1:]\n",
        "acoustic3 = (pd.read_csv(\"acoustic_3.csv\").to_numpy())[:,1:]\n",
        "acoustic4 = (pd.read_csv(\"acoustic_4.csv\").to_numpy())[:,1:]\n",
        "acoustic5 = (pd.read_csv(\"acoustic_5.csv\").to_numpy())[:,1:]\n",
        "\n",
        "label = ngram[:,-1]\n",
        "text_labels =text\n",
        "text = text[:, 0:-1]\n",
        "n_gram = ngram[:,1:-1]\n",
        "acoustic = acoustic[:,1:-1]\n",
        "\n",
        "sc = StandardScaler()\n",
        "pca = PCA(n_components=10)\n",
        "pca_ = PCA(n_components= 5)\n",
        "\n",
        "# sc.fit(acoustic)\n",
        "# acoustic = sc.transform(acoustic)\n",
        "# sc.fit(text)\n",
        "# text = sc.transform(text)\n",
        "# sc.fit(n_gram)\n",
        "# n_gram = sc.transform(n_gram)\n",
        "\n",
        "\n",
        "# pca.fit(acoustic)\n",
        "# acoustic = pca.transform(acoustic)\n",
        "# pca_.fit(text)\n",
        "# text = pca_.transform(text)\n",
        "# pca_.fit(n_gram)\n",
        "# n_gram = pca_.transform(n_gram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgdxMcIWnOPF"
      },
      "outputs": [],
      "source": [
        "svm = SVC(kernel='linear', random_state = 0, C=1)\n",
        "ann = MLPClassifier(max_iter=1000, activation='tanh', solver='sgd',alpha=0.7)\n",
        "kmeans = KNeighborsClassifier(n_neighbors = 9)\n",
        "rf = RandomForestClassifier(n_estimators = 30, random_state = 0)\n",
        "lr = LogisticRegression(random_state=0, max_iter = 1000)\n",
        "scoring = ['precision_macro', 'recall_macro','f1_macro']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ7WSP5ChgvU"
      },
      "source": [
        "##Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYDuD_biiFK2"
      },
      "outputs": [],
      "source": [
        "acoustic_rf = []\n",
        "acoustic_svm = []\n",
        "acoustic_lr = []\n",
        "acoustic_kmeans = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7jz_XMYhtE4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Assuming you have your data in a pandas DataFrame called 'df'\n",
        "np.random.seed(42)\n",
        "\n",
        "# Specify the number of folds\n",
        "num_folds = 5\n",
        "\n",
        "# Initialize the cross-validator\n",
        "kf = KFold(n_splits=num_folds, random_state=42, shuffle = True)\n",
        "\n",
        "# Create lists to store the training and testing datasets for each fold\n",
        "train_datasets = []\n",
        "test_datasets = []\n",
        "\n",
        "data = np.concatenate((text,acoustic), axis = 1)\n",
        "\n",
        "data1 = np.concatenate((text,acoustic1), axis = 1)\n",
        "data2 = np.concatenate((text,acoustic2), axis = 1)\n",
        "data3 = np.concatenate((text,acoustic3), axis = 1)\n",
        "data4 = np.concatenate((text,acoustic4), axis = 1)\n",
        "data5 = np.concatenate((text,acoustic5), axis = 1)\n",
        "\n",
        "# data = acoustic\n",
        "\n",
        "# data1 = acoustic1\n",
        "# data2 = acoustic2\n",
        "# data3 = acoustic3\n",
        "# data4 = acoustic4\n",
        "# data5 = acoustic5\n",
        "\n",
        "# labels = np.reshape(ngram[:,-1], (-1, 1))\n",
        "# data1 = np.concatenate((text,labels), axis = 1)\n",
        "# data2 = np.concatenate((text,labels), axis = 1)\n",
        "# data3 = np.concatenate((text,labels), axis = 1)\n",
        "# data4 = np.concatenate((text,labels), axis = 1)\n",
        "# data5 = np.concatenate((text,labels), axis = 1)\n",
        "\n",
        "# labels = np.reshape(ngram[:,-1], (-1, 1))\n",
        "# data1 = np.concatenate((n_gram,labels), axis = 1)\n",
        "# data2 = np.concatenate((n_gram,labels), axis = 1)\n",
        "# data3 = np.concatenate((n_gram,labels), axis = 1)\n",
        "# data4 = np.concatenate((n_gram,labels), axis = 1)\n",
        "# data5 = np.concatenate((n_gram,labels), axis = 1)\n",
        "\n",
        "# data = np.concatenate((n_gram, text,acoustic), axis = 1)\n",
        "\n",
        "# data1 = np.concatenate((n_gram, text,acoustic1), axis = 1)\n",
        "# data2 = np.concatenate((n_gram, text,acoustic2), axis = 1)\n",
        "# data3 = np.concatenate((n_gram, text,acoustic3), axis = 1)\n",
        "# data4 = np.concatenate((n_gram, text,acoustic4), axis = 1)\n",
        "# data5 = np.concatenate((n_gram, text,acoustic5), axis = 1)\n",
        "\n",
        "# Split the data into folds and store the indices\n",
        "fold = 1\n",
        "for train_index, test_index in kf.split(data):\n",
        "    # Obtain the training and testing datasets based on the indices\n",
        "    if fold==1:\n",
        "      train_data = data1[train_index]\n",
        "      test_data = data1[test_index]\n",
        "\n",
        "    if fold==2:\n",
        "      train_data = data2[train_index]\n",
        "      test_data = data2[test_index]\n",
        "\n",
        "    if fold==3:\n",
        "      train_data = data3[train_index]\n",
        "      test_data = data3[test_index]\n",
        "\n",
        "    if fold==4:\n",
        "      train_data = data4[train_index]\n",
        "      test_data = data4[test_index]\n",
        "\n",
        "    if fold==5:\n",
        "      train_data = data5[train_index]\n",
        "      test_data = data5[test_index]\n",
        "\n",
        "    # Append the datasets to the lists\n",
        "    train_datasets.append(train_data)\n",
        "    test_datasets.append(test_data)\n",
        "    fold += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpry5xxUhgaA"
      },
      "outputs": [],
      "source": [
        "scores = []\n",
        "results_arr = []\n",
        "for fold in range(5):\n",
        "  rf.fit(train_datasets[fold][:,:-1], train_datasets[fold][:,-1])\n",
        "  results = metrics.classification_report(test_datasets[fold][:,-1], rf.predict(test_datasets[fold][:,:-1]))\n",
        "  results_arr.append(results)\n",
        "  f1_score_macro = float(results.split()[21])\n",
        "  scores.append(f1_score_macro)\n",
        "  y_pred_simple = rf.predict(test_datasets[fold][:,:-1])\n",
        "  baseline_accuracy = bootstrap_score(test_datasets[fold][:,-1], y_pred_simple, metric=macro_f1, random_state=5 )\n",
        "  print(baseline_accuracy)\n",
        "print(scores)\n",
        "print(np.mean(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6bOlFNOZzaj"
      },
      "outputs": [],
      "source": [
        "#!pip install statkit\n",
        "from sklearn.metrics import f1_score\n",
        "from statkit.non_parametric import bootstrap_score\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Define the macro F1 score as a custom scorer\n",
        "macro_f1_score = make_scorer(f1_score, average='macro')\n",
        "\n",
        "def macro_f1(y_true, y_pred):\n",
        "    return f1_score(y_true, y_pred, average='macro')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CttROJanzvTz"
      },
      "outputs": [],
      "source": [
        "results = results_arr[3]\n",
        "print(results)\n",
        "print(float(results.split()[21]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eGLrT7gnUVs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare your data\n",
        "f1_scores = scores\n",
        "std_deviations = [np.std(scores),np.std(scores),np.std(scores),np.std(scores),np.std(scores)]\n",
        "\n",
        "# Set up the plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Define the x-axis positions for the bars\n",
        "x_pos = np.arange(len(f1_scores))\n",
        "\n",
        "# Plot the bars\n",
        "ax.bar(x_pos, f1_scores, yerr=std_deviations, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_xlabel('Fold')\n",
        "ax.set_ylabel('F1 Score')\n",
        "ax.set_title('F1 Scores for each fold')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(['Fold 1', 'Fold 2', 'Fold 3', 'Fold 4', 'Fold 5'])\n",
        "ax.yaxis.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MMy9kLcwhi4B"
      },
      "source": [
        "##Shap Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdB0rKhuoWjR"
      },
      "outputs": [],
      "source": [
        "text = pd.read_csv('features_hindi_labelled.csv').to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlnXcCoZpH_n"
      },
      "outputs": [],
      "source": [
        "text = pd.read_csv('features_all_english.csv').to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNWZgckzopOb"
      },
      "outputs": [],
      "source": [
        "label = text[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TIfEgb0j1Y_"
      },
      "outputs": [],
      "source": [
        "data = text[:, 1:-1]\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZFMLhK7PVoB"
      },
      "outputs": [],
      "source": [
        "scores = cross_validate(rf, x_train, y_train, cv=5,scoring=scoring)\n",
        "print('Precision macros:', np.mean(scores['test_precision_macro']))\n",
        "print('Recall macros:', np.mean(scores['test_recall_macro']))\n",
        "print('F1 macros:', np.mean(scores['test_f1_macro']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfJYlGrfrOKC"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "rf.fit(x_train, y_train)\n",
        "explainer = shap.TreeExplainer(rf)\n",
        "shap_values = explainer.shap_values(x_train)\n",
        "shap.summary_plot(shap_values[0], x_train, feature_names = ['mlum','ndwr','avg_sentence','num_sentences','unintell_prop','repProp','ecolalia','discourse1','discourse2','unexpected_words','metaphors','num_questions','ADJ','ADP','ADV','AUX','CCONJ','DET','INTJ','NOUN','NUM','PART','PRON','PROPN','SCONJ','VERB'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82TEFnt_N9W_"
      },
      "outputs": [],
      "source": [
        "svm.fit(x_train, y_train)\n",
        "print(metrics.classification_report(y_test, svm.predict(x_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIOHuKIFOR1s"
      },
      "outputs": [],
      "source": [
        "svm_explainer = shap.KernelExplainer(svm.predict, x_train)\n",
        "svm_shap_values = svm_explainer.shap_values(x_train)\n",
        "shap.summary_plot(svm_shap_values, x_train, feature_names = ['unintell_prop','discourse2','unexpected_words','avg_sentence','ADP','ADJ'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgOtO94shvpG"
      },
      "outputs": [],
      "source": [
        "scores = cross_validate(ann, x_train, y_train, cv=LeaveOneOut(),scoring=scoring)\n",
        "print('Precision macros:', np.mean(scores['test_precision_macro']))\n",
        "print('Recall macros:', np.mean(scores['test_recall_macro']))\n",
        "print('F1 macros:', np.mean(scores['test_f1_macro']))\n",
        "ann.fit(x_train, y_train)\n",
        "print(metrics.classification_report(y_test, ann.predict(x_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FWfeHJjIbvQ"
      },
      "outputs": [],
      "source": [
        "scores = cross_validate(rf, x_train, y_train, cv=LeaveOneOut(),scoring=scoring)\n",
        "print('Precision macros:', np.mean(scores['test_precision_macro']))\n",
        "print('Recall macros:', np.mean(scores['test_recall_macro']))\n",
        "print('F1 macros:', np.mean(scores['test_f1_macro']))\n",
        "rf.fit(x_train, y_train)\n",
        "print(metrics.classification_report(y_test, rf.predict(x_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HA2YAdmyMbGN"
      },
      "outputs": [],
      "source": [
        "!pip install shap\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeKur4zCL4dl"
      },
      "outputs": [],
      "source": [
        "svm_explainer = shap.KernelExplainer(svm.predict, x_train)\n",
        "svm_shap_values = svm_explainer.shap_values(x_train)\n",
        "shap.summary_plot(svm_shap_values, x_train, feature_names = ['spectralFlux_sma3_amean','loudness_sma3_stddevRisingSlope','loudness_sma3_percentile80.0','loudness_sma3_percentile20.0','loudness_sma3_amean','loudness_sma3_meanFallingSlope','loudness_sma3_stddevFallingSlope','equivalentSoundLevel_dBp','spectralFluxV_sma3nz_amean','spectralFluxUV_sma3nz_amean','loudness_sma3_percentile50.0','loudness_sma3_meanRisingSlope','loudness_sma3_pctlrange0-2','mfcc1_sma3_stddevNorm','mfcc1V_sma3nz_stddevNorm','MeanUnvoicedSegmentLength','mfcc1_sma3_amean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px_K3_7uNr-w"
      },
      "outputs": [],
      "source": [
        "scores = cross_validate(kmeans, x_train, y_train, cv=LeaveOneOut(),scoring=scoring)\n",
        "print('Precision macros:', np.mean(scores['test_precision_macro']))\n",
        "print('Recall macros:', np.mean(scores['test_recall_macro']))\n",
        "print('F1 macros:', np.mean(scores['test_f1_macro']))\n",
        "kmeans.fit(x_train, y_train)\n",
        "print(metrics.classification_report(y_test, kmeans.predict(x_test)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8AiCbTZnIFiu",
        "JKCuY0WWfRhR",
        "VilK-BqJOJs9",
        "V_9T-r_rj1Rc",
        "vmB7UybvxgAD",
        "VrRJhXAJBqdf",
        "5ZAOy9loCLHz",
        "ESU-8jIFCgZm",
        "7K_r6Sy0Ctnn",
        "Qqz2S8XkGkDW",
        "5CPK9kQsI097",
        "2Vo22IFw_hYj"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
